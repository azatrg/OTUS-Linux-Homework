# Bash, awk, sed, grep и другие

---

## Домашнее задание

Написать скрипт для крона
который раз в час присылает на заданную почту
- X IP адресов (с наибольшим кол-вом запросов) с указанием кол-ва запросов c момента последнего запуска скрипта
- Y запрашиваемых адресов (с наибольшим кол-вом запросов) с указанием кол-ва запросов c момента последнего запуска скрипта
- все ошибки c момента последнего запуска
- список всех кодов возврата с указанием их кол-ва с момента последнего запуска
в письме должно быть прописан обрабатываемый временной диапазон
должна быть реализована защита от мультизапуска
Критерии оценки: 5 - трапы и функции, а также sed и find +1 балл

---


## Решение

1. X IP адресов (с наибольшим кол-вом запросов) с указанием кол-ва запросов c момента последнего запуска скрипта

По умолчанию для ДЗ предлагается есть лог ./access-4560-644067.log , возьму его .

строка лог-файла имеет вид:

```
66.249.64.204 - - [15/Aug/2019:00:25:46 +0300] "GET / HTTP/1.1" 200 14446 "-" "Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.96 Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"rt=0.270 uct="0.000" uht="0.185" urt="0.270"
```
ip-адрес всегда первое слово в строке, соответственно есть несколько способов отобрать все адреса.

1й-способ -  используя *cut* с ключами d и f. С помошью d укажу что разделителем являеться пробел и сделаю выборку по первому полю.
```
cat access-4560-644067.log | cut -d ' ' -f 1
```
2й-способ с помощью *awk*. Суть та же -F указывает на разделитель. '{ print $1 }' на то какое по счету поле печатать.

```
awk -F " " '{ print $1 }' access-4560-644067.log
```
Далее для более удобного вывода отсортирую и оставлю унимальные значения. Для этого передам полученный ранее вывод через pipeline

```
awk -F " " '{ print $1 }' access-4560-644067.log | sort -n | uniq
```

Теперь надо посчитать сколько раз в логе отмечался каждый из полученных адресов. Для этого использую цикл for.

```
 for ip in $(awk -F " " '{ print $1 }' $FILE| sort -n | uniq);
 do { COUNT=`grep ^$ip $FILE |wc -l`;
 if [[ "$COUNT" -gt "5" ]]; then echo "$COUNT:   $ip" >> $OUTPUT;
 fi }; done
```

Для отправки email буду использовать утилиту mailx. Если ее нет в системе, то ставлю ее
```
sudo yum install -y mailx
```

Блок с отправкой email выглядит так:

```
##Send email

emailRecipient="root"
emailSubject="top requests from $FILE at $CURRENT_TIME"

cat $OUTPUT | sort -g | mailx -s "$emailSubject" "$emailRecipient"

```

2. Y запрашиваемых адресов (с наибольшим кол-вом запросов) с указанием кол-ва запросов c момента последнего запуска скрипта
Под запрашиваемыми адресами имеются ввиду страницы web-сервера.

Попробую все сделать аналогично адресам.

```
 for link in `cat $FILE |cut -d ' ' -f 7 |sort |uniq`;
 do { COUNT=`grep $link $FILE |wc -l`;
 if [[ "$COUNT" -gt "3" ]]; then echo "link $link hits $COUNT times " >> $OUTPUT;
 fi }; done
```
при таком подходе в выход попало значение "400", что не является url'ом. Просмотрев файл, понял что в некоторых местах поля съезжают и надо добавить дополнительную проверку. Для этого использую awk.

```
awk '($7 !~ /400/)' access-4560-644067.log| awk '{ print $7}' | sort -n | uniq
```
В этом примере я выбрал все строки, в которых поле №7 не равно 400. Затем вывел все на терминал и отсортировал. В процессе поиска нашел еще одно решение, в котором можно обойтись без цикла.
```
awk '($7 !~ /400/)' access-4560-644067.log| awk '{ print $7}' | sort -n | uniq -c | sort -nr | head -n 10
```



3. все ошибки c момента последнего запуска

Под ошибками понимаются коды ответов http. Пока в файле я встретил такие:
200
301
304
400
403
404
405
499
500
Ошибки это коды 400+ и 500+

За основу возьму тот же блок и немного его подредактирую.
Изменю условие, по которому коды попадают в блок, будут писаться только коды ошибок (те что равны\больше 400)
В итоге получился такой вариант

```
 for error in `cat $FILE |cut -d ' ' -f 9 |sort |uniq`;
 do { COUNT=`grep $error $FILE |wc -l`;
 if [[ "$error" -ge "400" ]]; then echo "$error code appears $COUNT times " >> $OUTPUT;
 fi }; done

cat $OUTPUT | sort -hr -k 4
```
**Вариант 2**
Понял что лучше изменить эту часть и поместить сюда не только коды ошибок и сколько раз они возникли, но и время, тип запросы, адреса клиента и url. Для этого воспользуюсь awk. Как в примере выше отберу те строки, в которых 9 поле не равно 200 и 301 (не являются ошибками) и выведу на печать нудные мне строки

```
awk '($9 !~ /200|301/ && $6 ~ /POST|GET|HEAD/)' access-4560-644067.log | awk '{print $1" "$4" "$5" "$6" "$7" "$8" "$9}'
```
но сюда не попали  ошибки 400, для этого добавлю еще одну строку
```
awk '($7 ~ /400/)' access-4560-644067.log | awk '{print}' 
```


4.  список всех кодов возврата с указанием их кол-ва с момента последнего запуска

Аналогично примеру выше.

---
В целом пока какой-никакой результат есть. Осталось исправить все ошибки. И решить следующие вопросы:
#### Решить следующие вопросы
1. Как отмечать ту часть файла которая была прочитана в прошлый раз?


2. Куда сохранять результат перед отправкой? 
## пока сохраняю во временный файл.

1. Как точно вычленить часть с адресом из строки. Если тупо с порядковым номером, то попадает лишнее( код ошибки). Надо как-то вытащить слово с помощью regexp.
https://markhneedham.com/blog/2013/06/26/unixawk-extracting-substring-using-a-regular-expression-with-capture-groups/ - вот пример




